{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\nimport re\\nimport sys\\nimport numpy as np\\nimport torch\\nimport dgl\\nfrom torch import nn\\nfrom torch.nn import functional as functional\\nfrom torch.utils.tensorboard import SummaryWriter\\nfrom tqdm import tqdm\\n\\nsys.path.append(\\\"../py\\\")\\nsys.path.append(\\\"../lib/BioInfer_software_1.0.1_Python3/\\\")\\n\\nfrom config import (\\n    ENTITY_PREFIX,\\n    PREDICATE_PREFIX,\\n    EPOCHS,\\n    WORD_EMBEDDING_DIM,\\n    VECTOR_DIM,\\n    HIDDEN_DIM,\\n    RELATION_EMBEDDING_DIM,\\n    BATCH_SIZE,\\n    MAX_LAYERS,\\n    MAX_ENTITY_TOKENS,\\n    CELL_STATE_CLAMP_VAL,\\n    HIDDEN_STATE_CLAMP_VAL,\\n)\\n\\nfrom bioinferdataset import BioInferDataset\\nfrom INN import INNModel\\nfrom utils import process_sample, get_child_indices\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\nimport re\\nimport sys\\nimport numpy as np\\nimport torch\\nimport dgl\\nfrom torch import nn\\nfrom torch.nn import functional as functional\\nfrom torch.utils.tensorboard import SummaryWriter\\nfrom tqdm import tqdm\\n\\nsys.path.append(\\\"../py\\\")\\nsys.path.append(\\\"../lib/BioInfer_software_1.0.1_Python3/\\\")\\n\\nfrom config import (\\n    ENTITY_PREFIX,\\n    PREDICATE_PREFIX,\\n    EPOCHS,\\n    WORD_EMBEDDING_DIM,\\n    VECTOR_DIM,\\n    HIDDEN_DIM,\\n    RELATION_EMBEDDING_DIM,\\n    BATCH_SIZE,\\n    MAX_LAYERS,\\n    MAX_ENTITY_TOKENS,\\n    CELL_STATE_CLAMP_VAL,\\n    HIDDEN_STATE_CLAMP_VAL,\\n)\\n\\nfrom bioinferdataset import BioInferDataset\\nfrom INN import INNModel\\nfrom utils import process_sample, get_child_indices\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "from torch import nn\n",
    "from torch.nn import functional as functional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../py\")\n",
    "sys.path.append(\"../lib/BioInfer_software_1.0.1_Python3/\")\n",
    "\n",
    "from config import (\n",
    "    ENTITY_PREFIX,\n",
    "    PREDICATE_PREFIX,\n",
    "    EPOCHS,\n",
    "    WORD_EMBEDDING_DIM,\n",
    "    VECTOR_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    RELATION_EMBEDDING_DIM,\n",
    "    BATCH_SIZE,\n",
    "    MAX_LAYERS,\n",
    "    MAX_ENTITY_TOKENS,\n",
    "    CELL_STATE_CLAMP_VAL,\n",
    "    HIDDEN_STATE_CLAMP_VAL,\n",
    ")\n",
    "\n",
    "from bioinferdataset import BioInferDataset\n",
    "from INN import INNModel\n",
    "from utils import process_sample, get_child_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"dataset = BioInferDataset(\\\"../data/BioInfer_corpus_1.1.1.xml\\\")\\n\\ntrain_idx = range(0, 880)\\nval_idx = range(880, 990)\";\n",
       "                var nbb_formatted_code = \"dataset = BioInferDataset(\\\"../data/BioInfer_corpus_1.1.1.xml\\\")\\n\\ntrain_idx = range(0, 880)\\nval_idx = range(880, 990)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = BioInferDataset(\"../data/BioInfer_corpus_1.1.1.xml\")\n",
    "\n",
    "train_idx = range(0, 880)\n",
    "val_idx = range(880, 990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabe/opt/miniconda3/envs/289G_NLP/lib/python3.8/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"model = INNModel(\\n    vocab_dict=dataset.vocab_dict,\\n    element_to_idx=dataset.element_to_idx,\\n    word_embedding_dim=WORD_EMBEDDING_DIM,\\n    relation_embedding_dim=RELATION_EMBEDDING_DIM,\\n    hidden_dim=HIDDEN_DIM,\\n    cell_state_clamp_val=CELL_STATE_CLAMP_VAL,\\n    hidden_state_clamp_val=HIDDEN_STATE_CLAMP_VAL,\\n)\\nparam_names = [p[0] for p in model.named_parameters()]\";\n",
       "                var nbb_formatted_code = \"model = INNModel(\\n    vocab_dict=dataset.vocab_dict,\\n    element_to_idx=dataset.element_to_idx,\\n    word_embedding_dim=WORD_EMBEDDING_DIM,\\n    relation_embedding_dim=RELATION_EMBEDDING_DIM,\\n    hidden_dim=HIDDEN_DIM,\\n    cell_state_clamp_val=CELL_STATE_CLAMP_VAL,\\n    hidden_state_clamp_val=HIDDEN_STATE_CLAMP_VAL,\\n)\\nparam_names = [p[0] for p in model.named_parameters()]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = INNModel(\n",
    "    vocab_dict=dataset.vocab_dict,\n",
    "    element_to_idx=dataset.element_to_idx,\n",
    "    word_embedding_dim=WORD_EMBEDDING_DIM,\n",
    "    relation_embedding_dim=RELATION_EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    cell_state_clamp_val=CELL_STATE_CLAMP_VAL,\n",
    "    hidden_state_clamp_val=HIDDEN_STATE_CLAMP_VAL,\n",
    ")\n",
    "param_names = [p[0] for p in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check not nan initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"for param in param_names:\\n    param = re.sub(r\\\"\\\\.([0-9])\\\", r\\\"[\\\\1]\\\", param)\\n    if torch.any(torch.isnan(eval(f\\\"model.{param}\\\"))):\\n        raise ValueError(f\\\"param {param} initialized with nans\\\")\";\n",
       "                var nbb_formatted_code = \"for param in param_names:\\n    param = re.sub(r\\\"\\\\.([0-9])\\\", r\\\"[\\\\1]\\\", param)\\n    if torch.any(torch.isnan(eval(f\\\"model.{param}\\\"))):\\n        raise ValueError(f\\\"param {param} initialized with nans\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for param in param_names:\n",
    "    param = re.sub(r\"\\.([0-9])\", r\"[\\1]\", param)\n",
    "    if torch.any(torch.isnan(eval(f\"model.{param}\"))):\n",
    "        raise ValueError(f\"param {param} initialized with nans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EPOCH 0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.92s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EPOCH 0 VALIDATION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:19<00:00,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Val Acc 0.6667 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"tb = SummaryWriter()\\n\\noptimizer = torch.optim.Adadelta(\\n    model.parameters(), lr=1.0\\n)  # TODO: changed to see if solves nan attn_scores weights\\ncriterion = nn.NLLLoss()\\n\\nEPOCHS = 1\\n\\ntorch.autograd.set_detect_anomaly(True)\\n\\nfor epoch in range(EPOCHS):\\n    print([f\\\"EPOCH {epoch}\\\"])\\n    for step in tqdm(train_idx[0:3]):  # TODO: remove, added for testing\\n        n_iter = (epoch) * len(train_idx) + step\\n        sample = process_sample(dataset[step], dataset.inverse_schema)\\n        optimizer.zero_grad()\\n        raw_predictions = model(\\n            sample[\\\"tokens\\\"],\\n            sample[\\\"entity_spans\\\"],\\n            sample[\\\"element_names\\\"],\\n            sample[\\\"H\\\"],\\n            sample[\\\"A\\\"],\\n            sample[\\\"T\\\"],\\n            sample[\\\"S\\\"],\\n        )\\n        predictions = torch.log(raw_predictions)\\n        loss = criterion(predictions, sample[\\\"labels\\\"])\\n        if loss.isnan().item():\\n            print(raw_predictions)\\n            raise ValueError(\\\"NaN loss encountered\\\")\\n\\n        # if the model has made predictions on relations.\\n        # This doesn't happen if there are no possible relations in the sentence given the schema\\n        if len(predictions) > len(sample[\\\"entity_spans\\\"]):\\n            loss.backward()\\n            optimizer.step()\\n            tb.add_scalar(\\\"loss\\\", loss, n_iter)\\n\\n        for param in param_names:\\n            param = re.sub(r\\\"\\\\.([0-9])\\\", r\\\"[\\\\1]\\\", param)\\n            tb.add_histogram(param, eval(f\\\"model.{param}\\\"), n_iter)\\n            tb.flush()\\n\\n    with torch.no_grad():\\n        val_accs = []\\n        print([f\\\"EPOCH {epoch} VALIDATION\\\"])\\n        for step in tqdm(val_idx[0:3]):  # TODO: remove, added for testing\\n            sample = process_sample(dataset[step], dataset.inverse_schema)\\n            labels = sample[\\\"labels\\\"]\\n            predictions = torch.argmax(\\n                model(\\n                    sample[\\\"tokens\\\"],\\n                    sample[\\\"entity_spans\\\"],\\n                    sample[\\\"element_names\\\"],\\n                    sample[\\\"H\\\"],\\n                    sample[\\\"A\\\"],\\n                    sample[\\\"T\\\"],\\n                    sample[\\\"S\\\"],\\n                )\\n            )\\n            acc = sum(predictions == labels) / len(labels)\\n            val_accs.append(acc.item())\\n\\n        val_acc = np.mean(val_accs)\\n        tb.add_scalar(\\\"val_acc\\\", val_acc, n_iter)\\n        print(\\\"Epoch {:05d} | Val Acc {:.4f} |\\\".format(epoch, val_acc))\\n        tb.flush()\";\n",
       "                var nbb_formatted_code = \"tb = SummaryWriter()\\n\\noptimizer = torch.optim.Adadelta(\\n    model.parameters(), lr=1.0\\n)  # TODO: changed to see if solves nan attn_scores weights\\ncriterion = nn.NLLLoss()\\n\\nEPOCHS = 1\\n\\ntorch.autograd.set_detect_anomaly(True)\\n\\nfor epoch in range(EPOCHS):\\n    print([f\\\"EPOCH {epoch}\\\"])\\n    for step in tqdm(train_idx[0:3]):  # TODO: remove, added for testing\\n        n_iter = (epoch) * len(train_idx) + step\\n        sample = process_sample(dataset[step], dataset.inverse_schema)\\n        optimizer.zero_grad()\\n        raw_predictions = model(\\n            sample[\\\"tokens\\\"],\\n            sample[\\\"entity_spans\\\"],\\n            sample[\\\"element_names\\\"],\\n            sample[\\\"H\\\"],\\n            sample[\\\"A\\\"],\\n            sample[\\\"T\\\"],\\n            sample[\\\"S\\\"],\\n        )\\n        predictions = torch.log(raw_predictions)\\n        loss = criterion(predictions, sample[\\\"labels\\\"])\\n        if loss.isnan().item():\\n            print(raw_predictions)\\n            raise ValueError(\\\"NaN loss encountered\\\")\\n\\n        # if the model has made predictions on relations.\\n        # This doesn't happen if there are no possible relations in the sentence given the schema\\n        if len(predictions) > len(sample[\\\"entity_spans\\\"]):\\n            loss.backward()\\n            optimizer.step()\\n            tb.add_scalar(\\\"loss\\\", loss, n_iter)\\n\\n        for param in param_names:\\n            param = re.sub(r\\\"\\\\.([0-9])\\\", r\\\"[\\\\1]\\\", param)\\n            tb.add_histogram(param, eval(f\\\"model.{param}\\\"), n_iter)\\n            tb.flush()\\n\\n    with torch.no_grad():\\n        val_accs = []\\n        print([f\\\"EPOCH {epoch} VALIDATION\\\"])\\n        for step in tqdm(val_idx[0:3]):  # TODO: remove, added for testing\\n            sample = process_sample(dataset[step], dataset.inverse_schema)\\n            labels = sample[\\\"labels\\\"]\\n            predictions = torch.argmax(\\n                model(\\n                    sample[\\\"tokens\\\"],\\n                    sample[\\\"entity_spans\\\"],\\n                    sample[\\\"element_names\\\"],\\n                    sample[\\\"H\\\"],\\n                    sample[\\\"A\\\"],\\n                    sample[\\\"T\\\"],\\n                    sample[\\\"S\\\"],\\n                )\\n            )\\n            acc = sum(predictions == labels) / len(labels)\\n            val_accs.append(acc.item())\\n\\n        val_acc = np.mean(val_accs)\\n        tb.add_scalar(\\\"val_acc\\\", val_acc, n_iter)\\n        print(\\\"Epoch {:05d} | Val Acc {:.4f} |\\\".format(epoch, val_acc))\\n        tb.flush()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb = SummaryWriter()\n",
    "\n",
    "optimizer = torch.optim.Adadelta(\n",
    "    model.parameters(), lr=1.0\n",
    ")  # TODO: changed to see if solves nan attn_scores weights\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print([f\"EPOCH {epoch}\"])\n",
    "    for step in tqdm(train_idx[0:3]):  # TODO: remove, added for testing\n",
    "        n_iter = (epoch) * len(train_idx) + step\n",
    "        sample = process_sample(dataset[step], dataset.inverse_schema)\n",
    "        optimizer.zero_grad()\n",
    "        raw_predictions = model(\n",
    "            sample[\"tokens\"],\n",
    "            sample[\"entity_spans\"],\n",
    "            sample[\"element_names\"],\n",
    "            sample[\"H\"],\n",
    "            sample[\"A\"],\n",
    "            sample[\"T\"],\n",
    "            sample[\"S\"],\n",
    "        )\n",
    "        predictions = torch.log(raw_predictions)\n",
    "        loss = criterion(predictions, sample[\"labels\"])\n",
    "        if loss.isnan().item():\n",
    "            print(raw_predictions)\n",
    "            raise ValueError(\"NaN loss encountered\")\n",
    "\n",
    "        # if the model has made predictions on relations.\n",
    "        # This doesn't happen if there are no possible relations in the sentence given the schema\n",
    "        if len(predictions) > len(sample[\"entity_spans\"]):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tb.add_scalar(\"loss\", loss, n_iter)\n",
    "\n",
    "        for param in param_names:\n",
    "            param = re.sub(r\"\\.([0-9])\", r\"[\\1]\", param)\n",
    "            tb.add_histogram(param, eval(f\"model.{param}\"), n_iter)\n",
    "            tb.flush()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_accs = []\n",
    "        print([f\"EPOCH {epoch} VALIDATION\"])\n",
    "        for step in tqdm(val_idx[0:3]):  # TODO: remove, added for testing\n",
    "            sample = process_sample(dataset[step], dataset.inverse_schema)\n",
    "            labels = sample[\"labels\"]\n",
    "            predictions = torch.argmax(\n",
    "                model(\n",
    "                    sample[\"tokens\"],\n",
    "                    sample[\"entity_spans\"],\n",
    "                    sample[\"element_names\"],\n",
    "                    sample[\"H\"],\n",
    "                    sample[\"A\"],\n",
    "                    sample[\"T\"],\n",
    "                    sample[\"S\"],\n",
    "                )\n",
    "            )\n",
    "            acc = sum(predictions == labels) / len(labels)\n",
    "            val_accs.append(acc.item())\n",
    "\n",
    "        val_acc = np.mean(val_accs)\n",
    "        tb.add_scalar(\"val_acc\", val_acc, n_iter)\n",
    "        print(\"Epoch {:05d} | Val Acc {:.4f} |\".format(epoch, val_acc))\n",
    "        tb.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
