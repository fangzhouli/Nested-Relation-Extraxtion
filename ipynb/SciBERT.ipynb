{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import re\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpha-catenin inhibits beta-catenin signaling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A binary complex of birch profilin and skeleta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abnormal immunoreactivity in the tumor tissue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absence of alpha-syntrophin leads to structura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abundance of actin , talin , alpha 5 and beta ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  alpha-catenin inhibits beta-catenin signaling ...\n",
       "1  A binary complex of birch profilin and skeleta...\n",
       "2  Abnormal immunoreactivity in the tumor tissue ...\n",
       "3  Absence of alpha-syntrophin leads to structura...\n",
       "4  Abundance of actin , talin , alpha 5 and beta ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ber = \"text_sentences.txt\"\n",
    "data = pd.read_csv(ber, delimiter='\\n', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences : 1100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total sentences :',len(data))\n",
    "max(data[0].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "def Bert_Tokens(sentence):\n",
    "        tokenized = tokenizer.encode_plus(\n",
    "                            text=sentence,  # the sentence to be encoded\n",
    "                            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "                            max_length = 40,  # maximum length of a sentence\n",
    "                            pad_to_max_length=True,  # Add [PAD]s\n",
    "                            #truncation = True,\n",
    "                        #     padding=True,\n",
    "                            return_attention_mask = True,  # Generate the attention mask\n",
    "                        #     return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "                        )\n",
    "        input_ids = torch.LongTensor([np.array(tokenized['input_ids'])])\n",
    "        attention_mask = torch.LongTensor([np.array(tokenized['attention_mask'])])\n",
    "        return input_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlpenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids,attention_mask = Bert_Tokens(data[0][0])\n",
    "print((input_ids.shape))\n",
    "print(type(input_ids))\n",
    "y = input_ids.squeeze(0)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = attention_mask.squeeze(0)\n",
    "a=torch.sum(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha-catenin', 'inhibits', 'beta-catenin', 'signaling', 'by', 'preventing', 'formation', 'of', 'a', 'beta-catenin*T-cell', 'factor*DNA', 'complex', '.']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "ty = data[0][0].split()\n",
    "print(ty)\n",
    "print(len(ty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'alpha', '-', 'cat', '##enin', 'inhibits', 'beta', '-', 'cat', '##enin', 'signaling', 'by', 'preventing', 'formation', 'of', 'a', 'beta', '-', 'cat', '##enin', '*', 't', '-', 'cell', 'factor', '*', 'dna', 'complex', '.', '[SEP]']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "text=data[0][0]\n",
    "marked_text = \"[CLS] \" + text+ \" [SEP]\"\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "tt = tokenized_text\n",
    "# Print out the tokens.\n",
    "print(tokenized_text)\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bert(seq_original, seq_bert):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def remove_leading_pounds(token):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        token_new = ''\n",
    "        for c in token:\n",
    "            if c != '#':\n",
    "                token_new += c\n",
    "        return token_new\n",
    "\n",
    "    # Remove header and footer tags.\n",
    "#     seq_bert = seq_bert[1:-1])\n",
    "    # Iterate over the original sequence and detect splitted tokens.\n",
    "    mapped_indices_list = []\n",
    "    j = 0\n",
    "    for i in range(len(seq_original)):\n",
    "        if seq_original[i] == seq_bert[j]:  # Not splitted.\n",
    "            j += 1\n",
    "            continue\n",
    "        else:  # Detect splitted tokens.\n",
    "            start = 0\n",
    "            token_splitted = seq_original[i]\n",
    "            token_mapping = remove_leading_pounds(seq_bert[j])\n",
    "            mapped_indices = []\n",
    "            while token_mapping == \\\n",
    "                    token_splitted[start : start + len(token_mapping)]:\n",
    "                mapped_indices.append(j)\n",
    "                start += len(token_mapping)\n",
    "                j += 1\n",
    "                token_mapping = remove_leading_pounds(seq_bert[j])\n",
    "            mapped_indices_list.append((i, mapped_indices))\n",
    "    return mapped_indices_list\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word sequence: ['alpha-catenin', 'inhibits', 'beta-catenin', 'signaling', 'by', 'preventing', 'formation', 'of', 'a', 'beta-catenin*t-cell', 'factor*dna', 'complex', '.']\n",
      "13\n",
      "BERT word sequence: ['alpha', '-', 'cat', '##enin', 'inhibits', 'beta', '-', 'cat', '##enin', 'signaling', 'by', 'preventing', 'formation', 'of', 'a', 'beta', '-', 'cat', '##enin', '*', 't', '-', 'cell', 'factor', '*', 'dna', 'complex', '.']\n",
      "28\n",
      "Splitted token: alpha-catenin\n",
      "Splitted to: alpha, -, cat, ##enin\n",
      "\n",
      "Splitted token: beta-catenin\n",
      "Splitted to: beta, -, cat, ##enin\n",
      "\n",
      "Splitted token: beta-catenin*t-cell\n",
      "Splitted to: beta, -, cat, ##enin, *, t, -, cell\n",
      "\n",
      "Splitted token: factor*dna\n",
      "Splitted to: factor, *, dna\n",
      "\n",
      "[(0, [0, 1, 2, 3]), (2, [5, 6, 7, 8]), (9, [15, 16, 17, 18, 19, 20, 21, 22]), (10, [23, 24, 25])]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'allenai/scibert_scivocab_uncased')\n",
    "\n",
    "text_test = data[0][0]\n",
    "seq_original = [w.lower() for w in data[0][0].split(' ')]\n",
    "seq_bert = tokenizer.tokenize(text_test )\n",
    "print(\"Original word sequence:\", seq_original)\n",
    "print(len(seq_original))\n",
    "print(\"BERT word sequence:\", seq_bert)\n",
    "print(len(seq_bert))\n",
    "\n",
    "splits = parse_bert(seq_original, seq_bert)\n",
    "for split in splits:\n",
    "    print(\"Splitted token:\", seq_original[split[0]])\n",
    "    print(\"Splitted to:\",\n",
    "        ', '.join([seq_bert[i] for i in split[1]]))\n",
    "    print()\n",
    "print((splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bert_New_Embedding(bert,split,shape):\n",
    "    '''\n",
    "    '''\n",
    "    print((split))\n",
    "    j=0\n",
    "    k=0\n",
    "    bert_new=torch.zeros([1,shape,768])\n",
    "    if len(split) != 0:\n",
    "        for i in range(len(split)):\n",
    "            while k< split[i][0]:\n",
    "                bert_new[:,k,:]=bert[:,j,:]\n",
    "                j += 1\n",
    "                k += 1\n",
    "            for p in range(len(split[i][1])):\n",
    "                j += 1\n",
    "\n",
    "            bert_new[:,k,:] = torch.sum(bert[:,split[i][1],:],dim = 1)/len(split[i][1])\n",
    "            k +=1\n",
    "        while k < shape:\n",
    "            bert_new[:,k,:]=bert[:,j,:]\n",
    "            j += 1\n",
    "            k += 1\n",
    "    else:\n",
    "        bert_new = bert\n",
    "        \n",
    "    return bert_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=config)\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(input_ids,attention_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_out=outputs1[0]\n",
    "bert_out = bert_out[:,0:a,:]\n",
    "bert_out = bert_out[:,1:-1,:]\n",
    "bert_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 1, 2, 3]), (2, [5, 6, 7, 8]), (9, [15, 16, 17, 18, 19, 20, 21, 22]), (10, [23, 24, 25])]\n"
     ]
    }
   ],
   "source": [
    "bert_out = Bert_New_Embedding(bert_out,splits,len(seq_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0830e-01,  1.3862e-01, -1.0345e+00, -5.0161e-01, -9.3059e-01,\n",
       "         9.3089e-02,  5.6875e-01,  3.6135e-01, -6.9744e-01,  4.3429e-01,\n",
       "         4.2599e-01,  2.0977e-02, -3.8835e-01,  3.2372e-02, -1.1121e+00,\n",
       "        -8.8309e-01,  5.9325e-01,  6.4571e-01,  1.4261e+00,  3.5278e-01,\n",
       "         6.8543e-02, -2.2690e-01,  5.4788e-02,  3.0494e-01, -5.1349e-01,\n",
       "        -8.2899e-02,  1.5260e-01, -7.5267e-02, -4.1532e-01,  1.8757e-01,\n",
       "         3.9923e-01,  4.6759e-01,  1.5293e-01, -1.0006e+00,  6.7551e-01,\n",
       "         2.5596e-01,  5.6533e-01, -4.4140e-02,  1.1028e-02, -6.6146e-02,\n",
       "        -5.1838e-01,  9.0952e-01,  1.2917e+00, -3.8168e-01,  6.5312e-01,\n",
       "         2.9474e-02,  2.7219e-01, -2.5619e-01, -3.7606e-01, -7.6582e-01,\n",
       "         3.6864e-01, -6.9876e-01, -4.5156e-01, -1.3754e-03, -3.7420e-01,\n",
       "         2.5871e-01, -4.3326e-01,  6.2785e-01,  3.2660e-02, -8.9519e-01,\n",
       "        -9.9723e-01,  2.1043e-02,  4.2238e-02,  1.7319e-01,  1.4061e+00,\n",
       "         7.7810e-02,  3.1586e-02,  8.3118e-02,  1.9547e-01,  1.0676e+00,\n",
       "         1.9854e-02, -4.1624e-01, -7.9016e-01, -5.5099e-01,  4.7250e-01,\n",
       "         9.6031e-01,  8.0437e-01,  6.7088e-01, -4.5457e-01,  3.9696e-01,\n",
       "         1.0082e+00, -6.3543e-01, -3.6847e-01,  2.9790e-01,  5.0216e-01,\n",
       "         1.4971e-01,  4.1917e-02,  6.8397e-02,  2.6589e-01,  5.0594e-01,\n",
       "         1.6067e-01, -1.1615e+00, -7.6316e-01, -5.3759e-01,  9.5358e-01,\n",
       "        -3.1572e-01, -3.1813e-01,  1.0192e+00,  8.5767e-02,  2.3034e-01,\n",
       "         7.0354e-01,  4.1612e-01,  2.0119e-01,  8.3246e-01,  5.8114e-01,\n",
       "        -7.7482e-01,  1.3319e-01,  8.7538e-03, -1.4563e-01,  6.7816e-01,\n",
       "        -3.6151e-01, -8.0074e-01, -2.4419e-01, -3.7083e-01, -4.0563e-01,\n",
       "        -2.9149e-01, -5.9983e-02,  1.9500e-01,  4.1500e-01,  8.9605e-01,\n",
       "        -4.7191e-01,  5.3288e-02,  5.4395e-01,  5.2891e-01, -2.5597e-01,\n",
       "         6.0636e-01,  6.7167e-01,  3.9605e-01,  8.2522e-01, -8.3550e-01,\n",
       "         1.3759e-01, -1.3975e+00,  6.0016e-01,  3.0843e-01, -3.6159e-01,\n",
       "        -9.5825e-01, -9.0224e-01, -1.2265e-03, -2.7969e-01, -4.4896e-01,\n",
       "        -9.8787e-01,  9.2086e-01, -5.0123e-01,  2.9577e-02,  9.3807e-01,\n",
       "         5.0285e-02,  5.7647e-01,  4.4276e-01, -1.5447e-01,  3.7763e-01,\n",
       "         9.2560e-02, -1.4489e-01, -6.2943e-01,  7.2056e-01,  1.0033e+00,\n",
       "        -3.4743e-01, -8.1669e-01, -2.8573e-02, -3.4562e-03,  1.6717e-01,\n",
       "         3.4216e-01, -1.2578e+00,  4.2457e-01,  2.1921e-01, -3.9003e-01,\n",
       "         1.1385e+00, -3.4501e-01,  1.2193e-01, -7.4790e-01, -1.5971e-01,\n",
       "         3.2816e-01, -2.1557e-01, -5.6660e-01,  5.3710e-01,  8.2419e-02,\n",
       "        -8.8871e-01, -8.4727e-01, -2.4624e-01, -4.1361e-01,  1.5690e-01,\n",
       "         3.6853e-01,  5.4567e-01, -6.9905e-01, -9.9426e-01, -9.1542e-01,\n",
       "        -4.0343e-01, -2.4030e-01, -4.2876e-01, -1.3964e-01, -8.4880e-01,\n",
       "         7.4592e-03, -3.1393e-01,  1.8994e-01, -9.6324e-01, -4.5401e-01,\n",
       "         1.0404e+00, -4.9243e-01,  3.8584e-01,  2.4325e-01, -3.6797e-01,\n",
       "        -7.9990e-01,  4.2912e-01, -2.6870e-01,  1.1093e+00, -3.5704e-01,\n",
       "         9.5376e-01,  5.8546e-01,  1.4807e-01, -8.7708e-01,  6.3860e-01,\n",
       "        -6.5991e-01, -2.9315e-01, -2.5686e-01, -1.0025e-01, -1.9279e-01,\n",
       "         1.4269e-01, -9.4171e-01, -1.2464e+00, -4.6517e-01, -9.7149e-01,\n",
       "         1.0524e-01, -5.2185e-01, -1.1044e-01, -1.7509e-01, -1.4800e-01,\n",
       "        -1.0604e+00,  6.0421e-01, -2.0523e-01,  7.4672e-01,  1.2133e-01,\n",
       "         3.7739e-01, -1.6960e-01,  6.8090e-01,  1.4821e-01,  1.7801e-01,\n",
       "        -1.3307e-01,  6.7641e-01, -3.6273e-01, -6.7457e-01, -1.0073e-01,\n",
       "         5.7881e-01,  7.7713e-01, -2.3423e-01,  2.0643e-01, -2.1871e-02,\n",
       "         1.6649e-01,  2.5716e-02, -2.2447e-02,  3.0298e-01, -6.8629e-01,\n",
       "        -5.4303e-01, -6.4802e-01, -5.2501e-01,  8.6794e-02, -2.4414e-01,\n",
       "         8.5642e-01,  3.3444e-01,  2.8989e-01,  7.2314e-01,  4.0190e-01,\n",
       "         3.1925e-02, -4.6058e-01,  3.6022e-01,  1.2356e+00,  5.1920e-01,\n",
       "        -5.7598e-01, -5.7875e-01,  5.1775e-01,  2.8689e-01,  6.0066e-01,\n",
       "        -3.7072e-01, -6.3510e-01, -4.1162e-01,  3.5770e-01, -7.3357e-02,\n",
       "         3.9861e-01,  4.6540e-01,  6.4321e-01,  4.3984e-01,  2.0520e-01,\n",
       "         6.1529e-01,  1.6844e-01,  9.8584e-02, -1.9299e-02,  5.5896e-02,\n",
       "        -1.0374e+00, -2.9032e-01, -1.0160e-01,  9.0913e-01, -1.2141e-01,\n",
       "        -2.5469e-01, -2.3516e-01,  4.0717e-01, -2.2850e-01,  4.7354e-01,\n",
       "        -2.7969e-01,  1.4074e-04, -5.5684e-01, -8.3191e-02, -3.9078e-01,\n",
       "         2.7700e-01, -6.0289e-01, -9.3095e-02,  6.4412e-01, -4.8213e-01,\n",
       "        -2.4160e-01, -1.5519e-01,  5.1616e-01, -1.9442e-01, -3.7666e-01,\n",
       "        -4.8597e-01, -6.7265e-01,  1.6503e-01, -4.8631e-02, -1.2610e+00,\n",
       "         1.0465e-03, -6.2457e-01,  1.5834e-01,  6.3902e-02, -1.1306e-02,\n",
       "        -3.8583e-01,  4.4119e-01, -1.2650e+00,  1.3087e-01,  1.7914e-01,\n",
       "         9.7234e-01, -1.2692e-01,  7.5720e-02,  1.3429e+00,  1.0141e-01,\n",
       "         4.9080e-01,  5.0935e-01, -1.6582e+00, -6.4148e-01,  1.6040e+00,\n",
       "        -2.9089e-01,  5.0369e-01,  2.5610e-01,  3.2974e-02, -1.7436e-01,\n",
       "        -1.3036e+00,  8.1293e-01,  3.2565e-01, -9.0589e-01,  1.8952e-01,\n",
       "        -2.0153e-01, -5.8293e-01,  2.3834e-01,  6.9478e-01, -8.0547e-02,\n",
       "         4.6992e-01,  1.2725e+00,  1.5380e-01, -1.8095e-01, -2.8732e-01,\n",
       "         4.0605e-01,  3.1035e-01, -3.3005e-01,  3.8764e-01,  2.1448e-01,\n",
       "        -4.6161e-01, -2.1197e-01, -3.4827e-01,  6.1950e-01,  1.0390e-01,\n",
       "         8.5357e-01,  9.4844e-01,  1.9824e-02, -1.4175e+00, -3.9069e-01,\n",
       "        -4.1352e-01,  1.7831e-01, -7.8762e-01,  2.0771e-01,  1.0372e-01,\n",
       "        -1.9522e-02, -2.0477e-01, -1.8409e-01,  1.1413e+00, -3.9458e-01,\n",
       "        -1.7343e-01, -2.8137e-01, -2.3193e-01, -5.8255e-01,  4.1985e-01,\n",
       "         2.4582e-01,  2.7352e-01, -2.4861e-01, -1.4763e-01, -5.0256e-01,\n",
       "         1.2837e+00,  2.2245e-01, -9.5960e-02,  9.3538e-01, -4.2401e-01,\n",
       "        -5.0934e-01,  5.8129e-02, -5.0445e-02, -1.8132e-01, -4.3850e-01,\n",
       "        -9.6051e-02, -6.9487e-01, -1.0078e+00,  7.0102e-01, -9.7168e-01,\n",
       "         1.9636e-01, -2.4925e-01,  1.6979e+00,  1.2575e-01,  7.0713e-02,\n",
       "         7.5153e-01, -6.4521e-02,  1.3638e-01, -2.8057e-01, -1.3543e-01,\n",
       "        -5.8849e-02, -2.5622e-01,  1.6142e-02,  5.2625e-01,  9.4896e-01,\n",
       "         2.0568e-01,  1.1758e+00,  1.1170e+01,  6.0035e-01, -1.5961e+00,\n",
       "        -7.7052e-01, -9.9817e-01, -1.9918e-01,  3.9899e-01,  4.8121e-02,\n",
       "        -5.7235e-02, -2.4761e-01, -1.2034e-01, -9.3702e-01, -2.9836e-01,\n",
       "         1.2012e+00,  4.3335e-01, -1.1850e+00, -6.5950e-01,  7.9736e-01,\n",
       "         3.4786e-01, -8.8359e-01, -7.6382e-01,  7.7665e-01, -5.8622e-02,\n",
       "         3.4645e-01, -3.5679e-01, -5.8745e-01,  5.4181e-01, -4.7665e-02,\n",
       "        -3.2102e-01,  4.3205e-01, -4.4378e-01, -7.0938e-02, -2.2545e-01,\n",
       "         5.6447e-01, -6.7853e-01, -2.1141e-01,  4.8294e-01,  3.1286e-01,\n",
       "         5.7218e-01,  5.3623e-01, -9.4242e-01, -1.0523e-01,  7.2549e-02,\n",
       "        -5.2144e-01,  3.4691e-01, -5.5022e-01,  3.4256e-01,  8.5829e-01,\n",
       "         1.1174e-01, -4.5341e-01,  1.5897e-01, -7.6271e-01,  4.2482e-01,\n",
       "         1.2352e-01,  2.9253e-01,  8.3911e-01,  1.9815e-01,  1.2623e-01,\n",
       "         1.4589e-02, -9.0526e-02,  5.3544e-01,  3.4887e-01, -1.7537e-01,\n",
       "         9.8514e-02, -4.0812e-01, -1.6765e-01, -3.3048e-01, -8.2947e-01,\n",
       "         7.2373e-01, -4.7554e-01, -3.4551e-01, -1.9694e-01, -8.1974e-02,\n",
       "         1.1158e+00, -7.0053e-01, -1.0234e+00,  6.4465e-01, -3.4791e-01,\n",
       "        -4.2553e-01, -2.2674e-01,  4.0536e-01, -2.1195e-01,  2.4059e-01,\n",
       "        -4.8522e-01,  3.2586e-03, -5.6906e-02, -7.4942e-01, -4.9694e-02,\n",
       "         1.5489e-01,  6.4940e-01,  5.3602e-01, -5.3431e-01, -5.2273e-02,\n",
       "         5.4207e-01, -9.7292e-01, -5.2349e-01,  7.1808e-01, -8.7423e-03,\n",
       "        -6.7917e-01,  4.9391e-01,  8.5809e-01,  8.5732e-01,  5.7124e-01,\n",
       "         3.7501e-02, -6.7635e-01, -7.2411e-02, -5.5488e-01, -1.8089e-01,\n",
       "        -9.0346e-02, -3.6311e-02, -8.7600e-01, -7.0179e-01,  1.3474e+00,\n",
       "        -5.1578e-01, -1.1234e-01, -7.6477e-01, -1.8676e-01,  3.0011e-01,\n",
       "        -9.6157e-01, -1.1127e-01,  7.9540e-01, -3.6391e-01, -3.5135e-01,\n",
       "        -9.1032e-01, -3.7356e-01,  6.6357e-01, -2.0253e-01, -4.5949e-01,\n",
       "        -1.8301e-01,  9.4207e-02,  4.5866e-02, -5.5272e-01, -3.9315e-01,\n",
       "         3.6892e-01,  4.8778e-01, -8.1913e-02,  8.9126e-01,  4.6841e-01,\n",
       "        -4.6038e-01, -1.0519e+00, -4.5539e-01, -6.7173e-01, -1.3419e+00,\n",
       "         8.6736e-02,  6.9697e-01,  2.1483e-01,  1.2571e-01, -6.1104e-02,\n",
       "        -2.6765e-01,  6.8421e-01,  1.1231e-01,  4.7498e-03, -1.5420e+00,\n",
       "         1.4275e+00, -3.4173e-01,  1.2436e+00, -1.3399e-01, -1.3497e-01,\n",
       "         5.8595e-01,  1.2020e+00, -4.7292e-01,  4.2618e-01, -3.0182e-01,\n",
       "        -5.6351e-01,  4.8091e-01,  1.4167e-01, -2.1070e-01, -5.2385e-01,\n",
       "         7.8336e-01, -6.5747e-01, -1.7616e-01, -4.0433e-01, -1.1404e+00,\n",
       "        -5.1045e-01, -1.9711e-01, -6.6656e-01,  7.4712e-01,  2.4221e-01,\n",
       "        -2.6201e-01, -5.8099e-01, -3.6136e-01,  4.2259e-02,  2.5265e-01,\n",
       "         1.0659e-01,  1.5929e+00, -4.1502e-01,  2.7896e-01,  1.2047e-01,\n",
       "         2.9622e-02, -2.4469e-01,  6.8475e-01,  5.9844e-01, -4.4255e-01,\n",
       "        -3.4438e-01,  2.7776e-01,  5.6526e-01, -1.7327e-01, -3.9946e-01,\n",
       "        -2.6302e-02, -5.6579e-02,  3.6067e-02,  5.6680e-01,  5.2919e-01,\n",
       "        -9.9902e-01,  5.2608e-01,  2.8033e-03,  7.8504e-02, -2.0247e-01,\n",
       "         3.0820e-01, -2.4622e-02, -1.1968e-01, -3.2458e-01,  7.6857e-02,\n",
       "         3.2136e-01,  6.7836e-01, -4.4233e-01, -2.6539e+00, -8.3935e-01,\n",
       "         3.8573e-01, -2.6030e-01, -9.0156e-01, -8.4114e-01, -6.1649e-01,\n",
       "        -2.8811e-01,  2.8832e-02,  3.5511e-01, -2.1651e-01,  8.1532e-02,\n",
       "        -2.6021e-01, -9.7165e-02,  1.0571e+00, -7.1096e-01,  4.7249e-01,\n",
       "        -5.8208e-01,  6.9106e-01,  2.3783e-01,  2.5232e-01, -2.0370e-01,\n",
       "        -8.1788e-01, -4.4662e-01,  5.4860e-01, -3.7965e-01, -2.1007e-02,\n",
       "         7.1446e-01, -2.7483e-01,  3.5138e-01,  5.9701e-01, -3.0870e-01,\n",
       "         2.5171e-02, -4.1933e-01,  2.9623e-02,  2.6021e-01, -1.5881e-01,\n",
       "        -7.0517e-01,  3.7508e-01,  4.3541e-01, -4.9291e-01,  6.2228e-02,\n",
       "         5.0973e-01,  8.9913e-01, -3.4093e-01,  2.5972e-01,  1.1369e-01,\n",
       "         1.8638e-01,  2.6573e-01, -1.8722e-01,  1.1740e-01, -7.2391e-01,\n",
       "        -1.6118e-01, -3.6279e-03, -6.3693e-01, -1.7965e-02, -2.0006e-01,\n",
       "        -4.6839e-01,  8.2823e-02,  4.1805e-01,  3.3964e-01, -5.8052e-01,\n",
       "        -8.1007e-01, -8.0245e-01,  4.9900e-01,  2.0670e-01, -3.2498e-02,\n",
       "        -1.4164e-01,  4.0663e-02, -2.6700e-01,  4.1708e-01,  8.3332e-01,\n",
       "         7.2411e-01, -9.1380e-02,  7.2458e-01,  4.6914e-01,  1.0293e-01,\n",
       "        -4.5085e-01, -6.3231e-01,  4.6052e-01, -1.8254e-01,  5.6736e-01,\n",
       "         3.0307e-01, -1.7833e-01, -7.4558e-02,  4.9048e-01, -3.6866e-01,\n",
       "         1.1753e+00,  1.0890e-01,  4.8213e-01, -7.7597e-01, -1.6141e+00,\n",
       "        -1.9643e-01,  9.0553e-01, -2.4046e-02,  7.1911e-01,  2.0538e-01,\n",
       "        -1.8768e-02, -4.0448e-01, -6.0805e-02,  8.6743e-01, -7.5165e-02,\n",
       "         2.1936e-02,  1.0379e-01,  7.7529e-01,  1.1163e-02, -8.8896e-02,\n",
       "         1.0460e+00,  9.4285e-02,  4.1370e-01,  7.9749e-01,  7.2493e-01,\n",
       "        -1.1939e+00, -2.1281e-01,  3.2914e-01,  2.6246e-01,  2.9709e-01,\n",
       "         6.7578e-01,  3.6348e-01, -2.4005e-01, -1.5682e-02,  4.5712e-01,\n",
       "        -1.3040e-01,  6.2923e-01, -5.1814e-01, -9.1967e-02, -2.1546e-01,\n",
       "        -6.9338e-01, -3.2285e-02, -2.7730e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_out[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5083)\n",
      "tensor(0.1386)\n",
      "tensor(-1.0345)\n",
      "tensor([-1.2065e-01, -1.1065e+00, -2.5464e-01, -5.7854e-01, -3.2624e-01,\n",
      "        -2.0647e-01,  7.0490e-01,  9.9409e-01, -6.3572e-01, -1.4784e-01,\n",
      "         1.0825e+00, -7.5510e-01, -1.5567e-01, -6.7080e-01, -1.8279e+00,\n",
      "        -2.4280e-01,  1.4367e+00,  8.8319e-02,  9.4183e-01,  4.0336e-01,\n",
      "         1.6213e-01, -1.7464e-03,  1.0651e+00, -5.9033e-01,  3.9550e-01,\n",
      "        -5.2400e-02,  1.0258e+00, -1.3702e-01, -5.4504e-01, -5.4080e-01,\n",
      "        -1.2052e+00, -4.5433e-01,  3.7176e-02, -8.8752e-01,  7.3869e-01,\n",
      "         2.2285e-01,  4.4025e-01,  1.3059e-01,  8.8312e-01, -5.7267e-01,\n",
      "         2.1058e-01,  1.0572e-03,  1.1544e+00, -2.7692e-01,  2.3384e+00,\n",
      "        -8.1864e-01,  9.5467e-01,  3.5287e-01, -8.5025e-01, -6.7571e-01,\n",
      "         1.5203e-01, -1.2396e+00, -1.9128e-01, -2.9623e-02,  1.5094e-01,\n",
      "        -2.2973e-01, -1.5330e+00, -5.1553e-01,  1.2345e-01, -5.7522e-01,\n",
      "        -1.4762e+00,  6.9819e-01, -1.1420e-01,  6.3110e-01,  8.3623e-01,\n",
      "         7.4520e-01,  6.3488e-01, -6.1528e-01, -2.1822e-01, -3.1390e-01,\n",
      "         2.3193e-01,  3.3434e-01,  3.8876e-01, -1.4363e+00,  8.9048e-01,\n",
      "         1.3377e+00,  1.0347e+00,  1.7369e-01,  1.3126e-01, -3.1792e-02,\n",
      "         3.7819e-01, -1.0905e+00, -2.4063e-01,  7.6439e-01,  2.3785e-01,\n",
      "        -8.0121e-02,  1.4034e-01,  4.0874e-01, -6.9485e-01,  1.6900e-01,\n",
      "         1.0716e-01, -1.0390e-01, -1.6174e+00, -2.9401e-01,  1.8977e+00,\n",
      "         2.0016e-01, -5.5760e-01,  5.4654e-01, -2.7267e-01,  2.7991e-01,\n",
      "         7.9030e-01, -7.2193e-01,  8.8869e-01,  3.4277e-01,  2.2378e-01,\n",
      "         1.8658e-01, -2.3205e-01,  8.3309e-01,  9.2505e-01,  8.1442e-01,\n",
      "        -5.0314e-01, -6.6778e-01, -2.2207e-02, -4.2887e-01, -4.5084e-02,\n",
      "        -5.5495e-01, -1.2144e-01, -1.5813e-01,  4.0613e-01,  5.1033e-01,\n",
      "        -2.6522e-02,  2.1675e-01,  4.8528e-01, -2.6257e-01, -8.5312e-01,\n",
      "         7.7656e-02,  5.8913e-02,  2.7019e-02,  7.3409e-01, -9.1495e-01,\n",
      "        -1.7292e-01,  1.0694e-01,  7.7213e-01,  1.0458e-01,  1.1376e-01,\n",
      "        -6.9720e-01, -2.8785e-03,  3.6650e-01, -1.9608e+00, -1.0388e+00,\n",
      "        -4.5454e-01, -3.4801e-01,  6.6834e-03,  3.2213e-01,  2.0304e+00,\n",
      "        -1.1450e-01, -5.6212e-01,  7.2154e-01, -3.0623e-01,  1.2869e-02,\n",
      "        -1.0990e-01, -5.9785e-01, -5.3408e-01,  3.9127e-01,  1.6762e-01,\n",
      "         4.7196e-01, -6.5926e-02,  8.6094e-01,  4.4434e-02,  8.9482e-01,\n",
      "         5.4474e-01, -3.7944e-01,  4.8776e-01,  2.8175e-01, -4.3407e-01,\n",
      "         9.5773e-02, -1.7584e-02,  1.5501e-01, -1.1148e+00, -5.2373e-01,\n",
      "         1.0062e+00, -3.2436e-01, -7.5125e-01, -1.0602e+00, -1.6030e-01,\n",
      "         1.7075e-01, -4.2380e-01,  3.2930e-01, -2.2005e-01,  8.0327e-01,\n",
      "         4.1478e-01,  1.2622e+00, -5.8895e-01, -6.4899e-01,  1.4152e-02,\n",
      "        -8.7483e-01,  1.4366e-02, -1.8149e-01, -4.7009e-01, -6.1900e-01,\n",
      "         9.4745e-01,  3.4881e-01,  2.5204e-01, -5.7660e-01, -4.3122e-01,\n",
      "         3.0716e-01,  3.4973e-01,  7.2510e-01, -2.1801e-01,  3.7837e-01,\n",
      "        -6.8178e-01, -3.2244e-02,  8.8359e-02,  3.2279e-01, -7.3111e-01,\n",
      "         1.1281e+00,  5.9402e-01,  4.9633e-02,  6.7947e-01,  5.9385e-01,\n",
      "        -8.4098e-01, -7.3130e-01, -5.6017e-01, -6.0249e-01, -3.2496e-01,\n",
      "         4.8405e-01, -3.0803e-01, -1.0124e+00,  1.1513e+00, -3.3867e-01,\n",
      "         5.3237e-01, -8.9912e-01, -7.2423e-01, -7.0316e-01,  5.5085e-01,\n",
      "        -1.1918e+00, -1.1592e-01,  6.8333e-02,  7.9791e-01,  1.3672e-01,\n",
      "         4.2130e-01,  1.2893e-01,  5.8541e-01,  1.9607e-01, -2.5653e-01,\n",
      "        -4.3600e-01,  7.5643e-02, -3.6799e-01, -1.9233e-01,  9.2988e-01,\n",
      "         3.1732e-01,  3.9795e-01, -1.5172e-01, -2.7181e-01,  1.5209e-01,\n",
      "        -5.9362e-01,  5.6233e-01,  4.6416e-01, -2.4735e-02, -9.0455e-01,\n",
      "        -7.3405e-01, -3.3483e-01, -4.8719e-01,  2.4186e-02,  8.2861e-03,\n",
      "         6.5220e-01,  6.7530e-01,  1.9938e-02,  1.5290e+00,  4.8430e-01,\n",
      "        -3.2047e-01, -5.0966e-01,  9.2615e-01,  7.9957e-02,  4.8671e-01,\n",
      "        -1.0433e+00, -5.7598e-01, -1.8392e-01, -4.5359e-01, -5.5231e-01,\n",
      "         4.1968e-01, -1.0268e+00,  1.9597e-01,  7.9218e-02, -3.3088e-01,\n",
      "        -6.4577e-01,  1.0661e+00,  7.5037e-01,  4.4599e-01,  4.3577e-01,\n",
      "         4.7722e-01,  1.0448e-01, -8.9070e-01,  5.7212e-02,  1.1602e-01,\n",
      "        -1.7083e+00, -2.4677e-02,  6.6486e-01,  1.1122e+00, -1.5198e-01,\n",
      "         5.6348e-01,  4.2925e-01, -4.8154e-01,  8.7537e-02,  5.2345e-01,\n",
      "        -4.0513e-01, -6.8334e-01, -9.0570e-01,  5.6238e-01,  2.3887e-01,\n",
      "        -5.2455e-01, -2.2651e-01,  6.8426e-01,  1.7993e-01, -2.8002e-01,\n",
      "        -1.4909e-01, -7.8717e-01,  5.3560e-01, -8.7361e-01,  2.8513e-01,\n",
      "        -6.9153e-01,  6.5708e-01, -3.7375e-01,  5.1330e-01, -8.3023e-01,\n",
      "         2.8735e-01, -1.7332e-01,  2.5228e-01, -7.5364e-01, -5.1348e-01,\n",
      "        -1.0325e+00, -7.5341e-02, -7.1773e-01,  8.2151e-01,  5.5386e-01,\n",
      "         1.0564e+00, -6.2191e-01,  3.1625e-01,  1.1083e-01,  7.3377e-01,\n",
      "         5.8511e-01,  4.9497e-01, -8.8326e-01, -3.6218e-01,  8.9220e-01,\n",
      "        -6.7258e-01,  9.7274e-02, -8.9079e-02,  5.4757e-01,  1.4372e-01,\n",
      "        -1.1499e+00, -7.2890e-02,  1.5631e-01,  1.2905e-01,  4.2030e-01,\n",
      "         4.0455e-01, -5.2065e-01, -2.3377e-02, -3.1771e-01,  2.3910e-01,\n",
      "         9.1100e-01,  1.5195e+00,  3.7396e-01, -6.7409e-01, -3.3514e-01,\n",
      "         2.5256e-01,  1.6048e-01, -2.6901e-01, -3.6546e-01, -4.1930e-02,\n",
      "        -2.6496e-02,  8.9477e-01, -8.2021e-01,  1.1023e-01,  4.6383e-01,\n",
      "         1.1344e-01,  1.6466e+00,  3.1434e-01, -3.8298e-02, -8.5251e-01,\n",
      "        -1.0581e+00,  2.4217e-01, -5.1596e-01,  8.4973e-02,  8.7764e-01,\n",
      "        -5.5242e-02, -8.3033e-01, -1.6325e-01,  5.9894e-01, -2.2926e-02,\n",
      "        -8.2050e-01,  1.1822e+00, -1.2944e+00, -1.6242e+00,  7.3704e-01,\n",
      "        -5.5347e-02, -1.6699e-03, -1.5796e+00,  5.9222e-02, -7.7207e-01,\n",
      "         1.4886e+00, -7.8659e-01, -4.0349e-02,  1.1116e+00, -2.6271e-01,\n",
      "        -5.5694e-01, -8.3052e-01, -5.4949e-01, -1.4360e-01, -8.3934e-01,\n",
      "         1.0786e+00, -3.4386e-01, -1.3183e+00,  2.0514e-01, -2.2453e+00,\n",
      "         1.6949e-01,  1.6737e-01,  9.5074e-01,  1.4081e+00,  1.3429e+00,\n",
      "         4.3541e-01, -2.2562e-02,  2.1606e-02, -7.4626e-01,  3.1250e-01,\n",
      "        -2.8403e-01, -2.6531e-01,  5.5922e-01, -4.0842e-01,  1.4768e+00,\n",
      "        -9.1978e-02,  5.6007e-01,  1.2702e+01,  2.8951e-01, -1.9565e+00,\n",
      "        -1.1743e+00, -1.8963e+00,  8.4447e-01,  4.2257e-01,  3.6841e-01,\n",
      "        -5.5336e-01,  2.0382e-01, -2.1119e-01, -2.2786e-01, -5.2996e-01,\n",
      "        -1.5077e-01, -2.1247e-02, -1.4551e+00, -4.1614e-01,  1.2302e+00,\n",
      "        -2.2568e-02, -9.9355e-01, -1.0568e+00,  6.1649e-01, -1.9863e-01,\n",
      "         4.3814e-01, -5.2219e-01, -1.7054e-01,  2.7226e-01, -1.1571e+00,\n",
      "         4.6606e-02,  8.3868e-01, -1.2718e+00, -4.1951e-01,  9.2733e-01,\n",
      "         5.3260e-01, -1.2766e+00, -8.0530e-01,  8.4734e-02,  3.6690e-01,\n",
      "        -4.3224e-01,  6.3060e-01, -1.1758e+00,  9.7260e-01,  1.0131e+00,\n",
      "        -1.2281e+00, -7.1085e-01, -4.9823e-01,  5.1335e-01,  8.3780e-01,\n",
      "        -3.2172e-01, -5.8136e-01, -6.0955e-01, -3.1962e-01,  1.9072e-01,\n",
      "        -5.0168e-01, -5.9542e-01,  5.6714e-01,  2.6743e-03,  3.4700e-01,\n",
      "        -3.5606e-02,  2.7545e-01,  2.7047e-01,  2.1003e-01,  9.3906e-01,\n",
      "         5.9632e-01, -6.4788e-01, -6.0651e-01, -3.0410e-01, -2.4375e-01,\n",
      "         6.8903e-01, -1.3631e-02, -1.3411e-01, -4.2964e-01,  8.3665e-01,\n",
      "        -1.8168e-01, -6.8751e-01, -1.0979e+00,  1.0448e+00, -2.6507e-01,\n",
      "         1.0786e+00, -7.8127e-02,  1.0385e+00, -1.7239e-02,  1.5991e+00,\n",
      "         1.2175e-01, -5.9223e-01,  4.1833e-01,  1.4804e-01,  4.2309e-01,\n",
      "         8.0121e-02,  6.4712e-01,  1.2293e+00, -8.0527e-01, -2.2570e-01,\n",
      "         2.5461e-01, -1.8536e+00,  2.5776e-01,  2.8103e-01,  1.1590e+00,\n",
      "        -1.1807e+00, -3.7907e-01,  5.1246e-01,  5.5571e-01,  3.6403e-01,\n",
      "         4.0525e-01, -8.5049e-01,  4.7687e-01, -1.5368e+00, -3.4589e-01,\n",
      "        -4.1841e-01,  4.0891e-01, -1.1998e+00, -4.1730e-01, -3.6616e-01,\n",
      "        -6.8180e-01,  1.7461e-01, -6.1294e-01, -6.1513e-02,  1.8368e-01,\n",
      "        -1.2496e+00,  3.5449e-01,  1.3074e+00, -6.0703e-01, -1.3603e+00,\n",
      "        -6.4488e-01, -3.1526e-01,  1.0769e+00, -7.7680e-01, -1.6023e-01,\n",
      "         3.2511e-01, -3.3568e-01, -4.0642e-01, -7.3383e-01, -1.6408e+00,\n",
      "         8.0426e-01,  8.2903e-01, -5.0740e-01,  2.8009e-01,  7.0634e-01,\n",
      "        -1.1354e+00, -1.2466e+00, -8.5727e-01, -1.5493e-01, -3.0302e-01,\n",
      "        -4.4697e-01, -8.2614e-02,  6.7100e-02,  7.8769e-01,  4.1119e-02,\n",
      "         1.9456e-01,  6.5745e-01,  1.0575e-01, -2.3793e-01, -1.6693e+00,\n",
      "         6.4455e-01, -2.6172e-01,  1.6075e+00,  2.1576e-02,  3.7563e-01,\n",
      "         4.8832e-01,  1.7116e+00,  1.0671e+00,  1.4279e-04,  3.6118e-01,\n",
      "        -3.0264e-01,  1.5490e+00, -8.5099e-01, -2.6368e-01, -5.5712e-01,\n",
      "         1.1253e+00,  1.9964e-01,  2.6102e-01,  9.2853e-01, -8.0089e-01,\n",
      "        -8.2792e-01, -2.3795e-01, -5.8557e-03,  1.4871e+00,  4.6721e-01,\n",
      "        -3.1602e-01, -1.1663e+00,  4.8368e-01, -5.4745e-01, -8.8422e-01,\n",
      "        -6.5029e-02,  9.9278e-01, -2.4381e-01,  6.4021e-01, -4.2780e-01,\n",
      "        -3.3236e-01, -8.3586e-01,  4.7058e-01, -5.9111e-01,  5.1658e-01,\n",
      "         4.5653e-01, -5.0729e-01,  1.1469e+00,  1.4827e-01, -1.2903e+00,\n",
      "         5.2058e-01,  3.2808e-01, -7.3438e-02, -7.7437e-01,  4.2039e-01,\n",
      "        -8.5247e-01,  6.1371e-01,  9.0396e-01,  1.2958e+00, -9.2564e-01,\n",
      "         9.5979e-01, -8.0311e-02,  2.1886e-01, -4.0613e-01,  1.7277e-02,\n",
      "        -1.4472e-01,  1.8839e+00, -4.9853e-01, -2.8187e+00, -5.2861e-01,\n",
      "         1.0761e+00,  6.3018e-01,  1.1511e-01, -8.6466e-01, -1.4771e+00,\n",
      "        -1.8475e-01, -1.4034e+00,  4.0838e-01, -7.5128e-01, -2.5423e-01,\n",
      "        -4.8664e-01,  1.9085e-02,  1.3954e+00, -7.1000e-02, -1.7801e-01,\n",
      "        -1.4339e+00,  4.9292e-01,  6.2808e-02,  2.4440e-01,  3.1756e-02,\n",
      "        -1.1680e+00, -9.0138e-02, -1.7833e-02, -5.2047e-01,  3.9396e-01,\n",
      "         4.1992e-01, -8.3840e-01, -5.4709e-01,  6.4661e-01,  2.7861e-01,\n",
      "        -1.0907e-01, -3.3378e-01,  1.0681e+00, -3.4221e-01, -1.5012e-01,\n",
      "        -1.1286e+00,  2.9103e-01,  9.0626e-02, -5.0449e-01, -3.4200e-02,\n",
      "         1.5753e-03, -4.8722e-01, -4.4631e-01,  9.9009e-02,  4.8569e-01,\n",
      "        -4.1702e-01,  6.6208e-01,  4.9950e-01,  6.7953e-01,  1.0867e-01,\n",
      "        -2.3425e-02, -1.9603e-01,  3.5598e-01,  1.2273e-01,  6.9132e-01,\n",
      "        -5.1181e-01, -8.7048e-01,  8.0063e-01, -3.8549e-01, -2.2189e-01,\n",
      "        -2.1985e+00, -4.2819e-01,  1.2256e+00, -8.9129e-01,  3.5114e-01,\n",
      "         3.2516e-01,  7.6501e-01, -1.6150e-03, -2.6226e-01,  5.7776e-02,\n",
      "         6.9471e-01, -4.1151e-02,  1.3047e+00, -8.6893e-01,  7.6005e-01,\n",
      "        -1.3088e-01, -7.7843e-01,  2.1261e-01, -3.1497e-01,  9.9488e-01,\n",
      "        -1.8800e-02, -1.5897e-01, -6.4439e-01,  1.0355e+00,  8.0785e-01,\n",
      "        -4.4978e-01,  9.7319e-01,  2.1216e-01, -1.2690e-01, -1.0272e+00,\n",
      "         1.4267e-01,  3.5988e-01, -5.4808e-01,  2.9434e-01, -1.2309e-01,\n",
      "        -4.3578e-01, -3.1878e-01,  6.5638e-02,  7.6234e-01,  4.0768e-01,\n",
      "        -2.4438e-01,  5.2941e-02, -1.6829e-02, -1.0972e+00,  4.0263e-01,\n",
      "         3.9038e-02,  1.2414e-01,  1.1180e+00,  3.0488e-01,  1.1255e+00,\n",
      "        -9.4435e-01,  9.7595e-01,  2.7927e-01,  3.7588e-02,  1.3311e+00,\n",
      "         8.7568e-01,  5.6934e-01,  2.8584e-01,  6.1762e-01, -6.8622e-02,\n",
      "         9.2503e-01,  5.6553e-01,  2.2256e-01, -8.0854e-01, -3.6001e-01,\n",
      "         3.5409e-02,  8.4949e-01, -7.2837e-01])\n"
     ]
    }
   ],
   "source": [
    "bt = outputs1[0]\n",
    "for i in range(3):\n",
    "    sm = (bt[0][24][i]+bt[0][25][i]+bt[0][26][i])/3\n",
    "    print(sm)\n",
    "print(bt[0][27])\n",
    "idx = [1, 2, 3, 4]\n",
    "ad = bt[:,idx,:].sum()/len(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
